"""
This type stub file was generated by pyright.
"""

def product1d(inrange):
    ...

def blocking(shape, block_shape, roi=..., center_blocks_at_roi=...):
    """ Generator for nd blocking.

    Args:
        shape (tuple): nd shape
        block_shape (tuple): nd block shape
        roi (tuple[slice]): region of interest (default: None)
        center_blocks_at_roi (bool): if given a roi,
            whether to center the blocks being generated
            at the roi's origin (default: False)
    """
    ...

def copy_dataset_impl(f_in, f_out, in_path_in_file, out_path_in_file, n_threads, chunks=..., block_shape=..., dtype=..., roi=..., fit_to_roi=..., **new_compression):
    """ Implementation of copy dataset.

    Used to implement `copy_dataset`, `convert_to_h5` and `convert_from_h5`.
    Can also be used for more flexible use cases, like copying from a zarr/n5
    cloud dataset to a filesytem dataset.

    Args:
        f_in (File): input file object.
        f_out (File): output file object.
        in_path_in_file (str): name of input dataset.
        out_path_in_file (str): name of output dataset.
        n_threads (int): number of threads used for copying.
        chunks (tuple): chunks of the output dataset.
            By default same as input dataset's chunks. (default: None)
        block_shape (tuple): block shape used for copying. Must be a multiple
            of ``chunks``, which are used by default (default: None)
        dtype (str): datatype of the output dataset, default does not change datatype (default: None).
        roi (tuple[slice]): region of interest that will be copied. (default: None)
        fit_to_roi (bool): if given a roi, whether to set the shape of
            the output dataset to the roi's shape
            and align chunks with the roi's origin. (default: False)
        **new_compression: compression library and options for output dataset. If not given,
            the same compression as in the input is used.
    """
    ...

def copy_dataset(in_path, out_path, in_path_in_file, out_path_in_file, n_threads, chunks=..., block_shape=..., dtype=..., use_zarr_format=..., roi=..., fit_to_roi=..., **new_compression):
    """ Copy dataset, optionally change metadata.

    The input dataset will be copied to the output dataset chunk by chunk.
    Allows to change chunks, datatype, file format and compression.
    Can also just copy a roi.

    Args:
        in_path (str): path to the input file.
        out_path (str): path to the output file.
        in_path_in_file (str): name of input dataset.
        out_path_in_file (str): name of output dataset.
        n_threads (int): number of threads used for copying.
        chunks (tuple): chunks of the output dataset.
            By default same as input dataset's chunks. (default: None)
        block_shape (tuple): block shape used for copying. Must be a multiple
            of ``chunks``, which are used by default (default: None)
        dtype (str): datatype of the output dataset, default does not change datatype (default: None).
        use_zarr_format (bool): file format of the output file,
            default does not change format (default: None).
        roi (tuple[slice]): region of interest that will be copied. (default: None)
        fit_to_roi (bool): if given a roi, whether to set the shape of
            the output dataset to the roi's shape
            and align chunks with the roi's origin. (default: False)
        **new_compression: compression library and options for output dataset. If not given,
            the same compression as in the input is used.
    """
    ...

def copy_group(in_path, out_path, in_path_in_file, out_path_in_file, n_threads):
    """ Copy group recursively.

    Copy the group recursively, using copy_dataset. Metadata of datasets that
    are copied cannot be changed and rois cannot be applied.

    Args:
        in_path (str): path to the input file.
        out_path (str): path to the output file.
        in_path_in_file (str): name of input group.
        out_path_in_file (str): name of output group.
        n_threads (int): number of threads used to copy datasets.
    """
    ...

class Timer:
    def __init__(self) -> None:
        ...
    
    @property
    def elapsed(self):
        ...
    
    def start(self):
        ...
    
    def stop(self):
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        ...
    


def fetch_test_data_stent():
    ...

def fetch_test_data():
    ...

def remove_trivial_chunks(dataset, n_threads, remove_specific_value=...):
    """ Remove chunks that only contain a single value.

    The input dataset will be copied to the output dataset chunk by chunk.
    Allows to change datatype, file format and compression as well.

    Args:
        dataset (z5py.Dataset)
        n_threads (int): number of threads
        remove_specific_value (int or float): only remove chunks that contain (only) this specific value (default: None)

    """
    ...

def remove_dataset(dataset, n_threads):
    """ Remvoe dataset multi-threaded.
    """
    ...

def remove_chunk(dataset, chunk_id):
    """ Remove a chunk
    """
    ...

def remove_chunks(dataset, bounding_box):
    """ Remove all chunks overlapping the bounding box
    """
    ...

def unique(dataset, n_threads, return_counts=...):
    """ Find unique values in dataset.

    Args:
        dataset (z5py.Dataset)
        n_threads (int): number of threads
        return_counts (bool): return counts of unique values (default: False)

    """
    ...

